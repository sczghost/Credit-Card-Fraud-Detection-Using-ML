# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R8CL6LK3Xqf2MqmVTP0Q4nJMpmuXAmxV
"""

# Credit Card Fraud Detection Project
# ----------------------------------
# Importing Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
import xgboost as xgb

from imblearn.over_sampling import SMOTE

# ----------------------------------
# Step 1: Load Dataset
# Replace with correct dataset path (creditcard.csv from Kaggle)
df = pd.read_csv("creditcard.csv")
print(df.head())
print(df["Class"].value_counts())

# ----------------------------------
# Step 2: Split Features & Target
X = df.drop("Class", axis=1)
y = df["Class"]

# Standardize features (important for LR, XGBoost)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

# ----------------------------------
# Step 3: Handle Imbalanced Data using SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("Before SMOTE:", y_train.value_counts())
print("After SMOTE:", y_train_smote.value_counts())

# ----------------------------------
# Step 4: Define Evaluation Function
def evaluate_model(model, X_test, y_test, model_name):
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)

    print(f"\n---- {model_name} ----")
    print(f"Accuracy: {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall (Sensitivity): {rec:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"ROC-AUC: {roc_auc:.4f}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(f"{model_name} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    plt.plot(fpr, tpr, label=f"{model_name} (AUC={roc_auc:.3f})")
    plt.plot([0, 1], [0, 1], "k--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve - {model_name}")
    plt.legend()
    plt.show()

# ----------------------------------
# Step 5: Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train_smote, y_train_smote)
evaluate_model(lr, X_test, y_test, "Logistic Regression")

# ----------------------------------
# Step 6: Decision Tree
dt = DecisionTreeClassifier(random_state=42, max_depth=5, min_samples_leaf=50)
dt.fit(X_train_smote, y_train_smote)
evaluate_model(dt, X_test, y_test, "Decision Tree")

# ----------------------------------
# Step 7: XGBoost Classifier
xgb_model = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=4,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    use_label_encoder=False,
    eval_metric="logloss"
)
xgb_model.fit(X_train_smote, y_train_smote)
evaluate_model(xgb_model, X_test, y_test, "XGBoost Classifier")

# ----------------------------------
# Step 8: Compare Model Performance
models = ["Logistic Regression", "Decision Tree", "XGBoost"]